{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc15ba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tkinter import *\n",
    "from tkinter import ttk\n",
    "import tkinter.font as tkFont\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a1d2e886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sigmoid (a,v):\n",
    "    temp=-1*a*v\n",
    "    result=1/(1+math.exp(temp))\n",
    "    return result\n",
    "\n",
    "\n",
    "def Tanh (a,v):\n",
    "    temp=1*a*v\n",
    "    #result=(1-math.exp(temp))/(1+math.exp(temp))\n",
    "    result=np.tanh(temp)\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b2c7ba6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(name):\n",
    "    if name == 'Iris-setosa':\n",
    "        return [1,0,0]\n",
    "    if name == 'Iris-versicolor':\n",
    "        return [0,1,0]\n",
    "    else :\n",
    "        return [0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4bc3af22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Initialize_weights(num_layers,num_neuron,bais): \n",
    "    if bais ==False :\n",
    "        first_w=np.random.rand(4,num_neuron[0])\n",
    "        hidden_w=[]\n",
    "        hidden_w.append(first_w)\n",
    "        for i in range(1,num_layers):\n",
    "            hidden=np.random.rand(num_neuron[i],num_neuron[i-1])\n",
    "            hidden_w.append(hidden)\n",
    "\n",
    "    \n",
    "        ouput_w=np.random.rand(3,num_neuron[num_layers-1])\n",
    "    \n",
    "        return hidden_w,ouput_w\n",
    "    else :\n",
    "        first_w=np.random.rand(5,num_neuron[0])\n",
    "        hidden_w=[]\n",
    "        hidden_w.append(first_w)\n",
    "        \n",
    "        for i in range(1,num_layers):\n",
    "            hidden=np.random.rand(num_neuron[i],num_neuron[i-1]+1)\n",
    "            hidden_w.append(hidden)\n",
    "            \n",
    "        ouput_w=np.random.rand(3,num_neuron[num_layers-1]+1)\n",
    "        return hidden_w,ouput_w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6332d8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicetion(hidden_w,ouput_w,sample,bais,user_AvtivationFunction,b,num_layers,num_neuron,stander_division):\n",
    "    x1=sample['X1'].min()\n",
    "    x2=sample['X2'].min()\n",
    "    x3=sample['X3'].min()\n",
    "    x4=sample['X4'].min()\n",
    "    \n",
    "    if bais == False :\n",
    "        ## calc z for hidden\n",
    "        z=[]     \n",
    "        for l in range(num_layers):\n",
    "            if l == 0 :\n",
    "                temp=[]\n",
    "                for neuron in range(num_neuron[0]):\n",
    "                    w=hidden_w[0] ## get weight of first layer and input layer\n",
    "                    result=x1*w[0,neuron]+x2*w[1,neuron]+x3*w[2,neuron]+x4*w[3,neuron] ##calc net\n",
    "                    if user_AvtivationFunction== 'Sigmoid':\n",
    "                        temp.append(Sigmoid(stander_division,result)) ## calc z output of activation function\n",
    "                    else :\n",
    "                        temp.append(stander_division*Tanh(b,result))\n",
    "                #print(temp,'input')\n",
    "                z.append(temp)\n",
    "\n",
    "            else :\n",
    "                temp=[]\n",
    "                temp_2=z[l-1]  ## get  z of previous layer\n",
    "                #print(temp_2)\n",
    "                for neuron in range(num_neuron[l]):\n",
    "                    w=hidden_w[l] ## array of weight that connected between this and previous layer\n",
    "                    sum=0\n",
    "                    for connection in range(num_neuron[l-1]):#### loop previous connections\n",
    "                        sum=sum+temp_2[connection]*w[neuron,connection]##note col is weights that connect to this neuron\n",
    "\n",
    "                    if user_AvtivationFunction== 'Sigmoid':\n",
    "                        temp.append(Sigmoid(stander_division,sum)) ## calc z output of activation function\n",
    "                    else :\n",
    "                        temp.append(stander_division*Tanh(b,sum))\n",
    "\n",
    "                z.append(temp)\n",
    "\n",
    "\n",
    "\n",
    "        ## calc y for output\n",
    "        y=[]\n",
    "        temp_2=z[-1]  ## get  z of last layer\n",
    "        for output in range(3):\n",
    "            w=ouput_w\n",
    "            sum=0\n",
    "            for connection in range(num_neuron[-1]):\n",
    "                sum=sum+temp_2[connection]*w[output,connection]\n",
    "\n",
    "            if user_AvtivationFunction== 'Sigmoid':\n",
    "                y.append(Sigmoid(stander_division,sum))\n",
    "            else :\n",
    "                y.append(stander_division*Tanh(b,sum))\n",
    "   \n",
    "    ##################################### with bais ########################\n",
    "    else :\n",
    "        ## calc z for hidden\n",
    "        z=[] \n",
    "\n",
    "        for l in range(num_layers):\n",
    "\n",
    "            if l == 0 :\n",
    "                temp=[]\n",
    "                for neuron in range(num_neuron[0]):\n",
    "                    w=hidden_w[0] ## get weight of first layer and input layer\n",
    "                    result=w[0,neuron]+x1*w[1,neuron]+x2*w[2,neuron]+x3*w[3,neuron]+x4*w[4,neuron] ##calc net\n",
    "                    if user_AvtivationFunction== 'Sigmoid':\n",
    "                        temp.append(Sigmoid(stander_division,result)) ## calc z output of activation function\n",
    "                    else :\n",
    "                        temp.append(stander_division*Tanh(b,result))\n",
    "                        \n",
    "                z.append(temp)\n",
    "\n",
    "            else :\n",
    "                temp=[]\n",
    "                temp_2=z[l-1]  ## get  z of previous layer\n",
    "                for neuron in range(num_neuron[l]):\n",
    "                    w=hidden_w[l] ## array of weight that connected between this and previous layer\n",
    "\n",
    "                    sum=0\n",
    "                    for connection in range(num_neuron[l-1]+1):#### loop previous connections\n",
    "                        if connection == 0 :\n",
    "                            sum=sum+w[neuron,connection]\n",
    "                        else :\n",
    "                            sum=sum+temp_2[connection-1]*w[neuron,connection]##note col is weights that connect to this neuron\n",
    "\n",
    "                    if user_AvtivationFunction== 'Sigmoid':\n",
    "                        temp.append(Sigmoid(stander_division,sum)) ## calc z output of activation function\n",
    "                    else :\n",
    "                        temp.append(stander_division*Tanh(b,sum))\n",
    "\n",
    "                z.append(temp)\n",
    "\n",
    "        ## calc y for output\n",
    "        y=[]\n",
    "        temp_2=z[-1]  ## get  z of last layer\n",
    "\n",
    "        for output in range(3):\n",
    "            w=ouput_w\n",
    "            sum=0\n",
    "            for connection in range(num_neuron[-1]+1):\n",
    "                if connection == 0 :\n",
    "                    sum=sum+w[output,connection]\n",
    "                else :\n",
    "                    sum=sum+temp_2[connection-1]*w[output,connection]\n",
    "            if user_AvtivationFunction== 'Sigmoid':\n",
    "                y.append(Sigmoid(stander_division,sum))\n",
    "            else :\n",
    "                y.append(stander_division*Tanh(b,sum))\n",
    "\n",
    "\n",
    "    #print(y)\n",
    "    if y[0]>y[1] :\n",
    "        if y[0]>y[2]:\n",
    "            return [1,0,0]\n",
    "        else :\n",
    "            return [0,0,1]\n",
    "    else :\n",
    "        if y[1]>y[2]:\n",
    "            return [0,1,0]\n",
    "        else :\n",
    "            return [0,0,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "26e5a21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(stander_division,epoches,train_data,b,learning_rate,hidden_w,\n",
    "                     ouput_w,bais,num_layers,num_neuron,user_AvtivationFunction):\n",
    "    if bais == False :\n",
    "        for epoch in range(epoches):\n",
    "            for i in range(int(train_data.size/5)):## should loop at 90 \n",
    "                print(epoch,end = \"\\r\")\n",
    "                ## get row from iris\n",
    "                samples=train_data[i:i+1]\n",
    "                \n",
    "                x1=samples['X1'].min()\n",
    "                x2=samples['X2'].min()\n",
    "                x3=samples['X3'].min()\n",
    "                x4=samples['X4'].min()\n",
    "                target=decode(samples['Class'].min())\n",
    "                \n",
    "                ##################################### feedforward 1 #####################################\n",
    "\n",
    "                ## calc z for hidden\n",
    "                z=[] \n",
    "\n",
    "                for l in range(num_layers):\n",
    "                    if l == 0 :\n",
    "                        temp=[]\n",
    "                        for neuron in range(num_neuron[0]):\n",
    "                            w=hidden_w[0] ## get weight of first layer and input layer\n",
    "                            result=x1*w[0,neuron]+x2*w[1,neuron]+x3*w[2,neuron]+x4*w[3,neuron] ##calc net\n",
    "                            if user_AvtivationFunction== 'Sigmoid':\n",
    "                                temp.append(Sigmoid(stander_division,result)) ## calc z output of activation function\n",
    "                            else :\n",
    "                                temp.append(stander_division*Tanh(b,result))\n",
    "                        z.append(temp)\n",
    "\n",
    "                    else :\n",
    "                        temp=[]\n",
    "                        temp_2=z[l-1]  ## get  z of previous layer\n",
    "                        for neuron in range(num_neuron[l]):\n",
    "                            w=hidden_w[l] ## array of weight that connected between this and previous layer\n",
    "\n",
    "                            sum=0\n",
    "                            for connection in range(num_neuron[l-1]):#### loop previous connections\n",
    "                                sum=sum+temp_2[connection]*w[neuron,connection]##note col is weights that connect to this neuron\n",
    "\n",
    "                            if user_AvtivationFunction== 'Sigmoid':\n",
    "                                temp.append(Sigmoid(stander_division,sum)) ## calc z output of activation function\n",
    "                            else :\n",
    "                                temp.append(stander_division*Tanh(b,sum))\n",
    "                                \n",
    "                        z.append(temp)\n",
    "                        \n",
    "                #print('z',z)        \n",
    "                ## calc y for output\n",
    "                y=[]\n",
    "                temp_2=z[-1]  ## get  z of last layer\n",
    "                for output in range(3):\n",
    "                    w=ouput_w\n",
    "\n",
    "                    sum=0\n",
    "                    for connection in range(num_neuron[-1]):\n",
    "                        sum=sum+temp_2[connection]*w[output,connection]\n",
    "                        \n",
    "                        \n",
    "                    if user_AvtivationFunction== 'Sigmoid':\n",
    "                        y.append(Sigmoid(stander_division,sum))\n",
    "                    else :\n",
    "                        y.append(stander_division*Tanh(b,sum))\n",
    "                    \n",
    "\n",
    "                \n",
    "                ##################################### back-propagation  #####################################\n",
    "\n",
    "\n",
    "                ## calc sk for output\n",
    "                sk=[]\n",
    "                #print('t',target)\n",
    "                #print('y',y)\n",
    "                for j in range(3):\n",
    "                    if user_AvtivationFunction== 'Sigmoid':\n",
    "                        sk.append(  stander_division*(target[j]-y[j])*y[j]*(1-y[j]) )  ## for segmoid function\n",
    "                    else :\n",
    "                        sk.append( ( b/stander_division)*(target[j]-y[j])*(stander_division-y[j])\n",
    "                                                       *(stander_division+y[j]))\n",
    "\n",
    "                #print('sk',sk)\n",
    "                ## calc sj for hidden layers\n",
    "                sj=[]\n",
    "                for layer in range(num_layers,0,-1): ## calc sj for each neurons in hidden layers\n",
    "\n",
    "                    if layer == num_layers : ## layer connent to output \n",
    "                        temp=[]\n",
    "                        temp_2=z[num_layers-1]\n",
    "                        for neuron in range(num_neuron[num_layers-1]):#calc sj for each neuron in last layer\n",
    "                            zj=temp_2[neuron]\n",
    "\n",
    "                            w=ouput_w\n",
    "                            sum=0\n",
    "                            for conn in range(3):\n",
    "                                sum=sum+w[conn,neuron]*sk[conn]\n",
    "                            #print('sum',sum)\n",
    "                            if user_AvtivationFunction== 'Sigmoid':\n",
    "                                temp.append( stander_division*zj*(1-zj)*sum ) ## sigmoid activation\n",
    "                            else :\n",
    "                                temp.append((b/stander_division)*(stander_division-zj)\n",
    "                                            *(stander_division+zj)* sum) ## tanh activation\n",
    "                            \n",
    "                        sj.append(temp)\n",
    "                        \n",
    "                        #print('sj',sj)\n",
    "\n",
    "                    else :\n",
    "                        temp=[]\n",
    "                        temp_2=z[layer-1]\n",
    "                        temp_3=sj[num_layers-layer-1]\n",
    "                        for neuron in range(num_neuron[layer-1]):#calc sj for each neuron in layer\n",
    "                            zj=temp_2[neuron]\n",
    "\n",
    "                            w=hidden_w[layer]\n",
    "                            sum=0\n",
    "                            for conn in range(num_neuron[layer]):\n",
    "                                sum=sum+w[conn,neuron]*temp_3[conn]\n",
    "                                \n",
    "                            if user_AvtivationFunction== 'Sigmoid':\n",
    "                                temp.append( stander_division*zj*(1-zj)*sum ) ## sigmoid activation\n",
    "                            else :\n",
    "                                temp.append((b/stander_division)*(stander_division-zj)\n",
    "                                            *(stander_division+zj)* sum) ## tanh activation\n",
    "\n",
    "                        sj.append(temp)\n",
    "                        \n",
    "                #print('sj',sj)\n",
    "                ##################################### feedforward (updata weights) 2  #########################\n",
    "                for layer in range(num_layers+1): ##loop at every layer\n",
    "                    if layer == num_layers:\n",
    "                        w=ouput_w\n",
    "                        temp_2=z[num_layers-1]\n",
    "                        for neuron in range(num_neuron[num_layers-1]):# for each neuron in last layer\n",
    "\n",
    "                            for conn in range(3):\n",
    "                                ouput_w[conn,neuron]= w[conn,neuron]+learning_rate*sk[conn]*temp_2[neuron]\n",
    "                                #ouput_w[conn,neuron]=1\n",
    "                        \n",
    "                    \n",
    "                    elif layer == 0 :\n",
    "                        #print('>>>>>>>>')\n",
    "                        #print(hidden_w[layer])\n",
    "                        w=hidden_w[layer]\n",
    "                        temp=sj[num_layers-layer-1] ## because sj in order inverse \n",
    "                        #print(temp)\n",
    "                        for neuron in range(num_neuron[layer]):# for each neuron in last layer \n",
    "                            w[0,neuron]=w[0,neuron]+learning_rate*temp[neuron]*x1\n",
    "                            w[1,neuron]=w[1,neuron]+learning_rate*temp[neuron]*x2\n",
    "                            w[2,neuron]=w[2,neuron]+learning_rate*temp[neuron]*x3\n",
    "                            w[3,neuron]=w[3,neuron]+learning_rate*temp[neuron]*x4\n",
    "                        hidden_w[layer]=w\n",
    "                        #print('<<<<<<')\n",
    "                        #print(w)\n",
    "                        \n",
    "                    else :\n",
    "                        w=hidden_w[layer]\n",
    "                        temp_2=z[layer-1]\n",
    "                        temp=sj[num_layers-layer-1]\n",
    "                        #print(temp,\" sj \")\n",
    "                        #print(temp_2,\" z \")\n",
    "                        for neuron in range(num_neuron[layer]):# for each neuron in hidden layer\n",
    "\n",
    "                            for conn in range(num_neuron[layer-1]):\n",
    "                                w[neuron,conn]=w[neuron,conn]+learning_rate*temp[neuron]*temp_2[conn]\n",
    "                        hidden_w[layer]=w\n",
    "                        \n",
    "    else:  ############ with bais\n",
    "        for epoch in range(epoches):\n",
    "            for i in range(int(train_data.size/5)):## should loop at 90\n",
    "                print(epoch,end = \"\\r\")\n",
    "                ## get row from iris\n",
    "                samples=train_data[i:i+1]\n",
    "\n",
    "                x1=samples['X1'].min()\n",
    "                x2=samples['X2'].min()\n",
    "                x3=samples['X3'].min()\n",
    "                x4=samples['X4'].min()\n",
    "                target=decode(samples['Class'].min())\n",
    "\n",
    "                ##################################### feedforward 1 #####################################\n",
    "\n",
    "                ## calc z for hidden\n",
    "                z=[] \n",
    "\n",
    "                for l in range(num_layers):\n",
    "                    \n",
    "                    if l == 0 :\n",
    "                        temp=[]\n",
    "                        for neuron in range(num_neuron[0]):\n",
    "                            w=hidden_w[0] ## get weight of first layer and input layer\n",
    "                            result=w[0,neuron]+x1*w[1,neuron]+x2*w[2,neuron]+x3*w[3,neuron]+x4*w[4,neuron] ##calc net\n",
    "                            if user_AvtivationFunction== 'Sigmoid':\n",
    "                                temp.append(Sigmoid(stander_division,result)) ## calc z output of activation function\n",
    "                            else :\n",
    "                                temp.append(stander_division*Tanh(b,result))\n",
    "                            \n",
    "                        z.append(temp)\n",
    "\n",
    "                    else :\n",
    "                        temp=[]\n",
    "                        temp_2=z[l-1]  ## get  z of previous layer\n",
    "                        for neuron in range(num_neuron[l]):\n",
    "                            w=hidden_w[l] ## array of weight that connected between this and previous layer\n",
    "\n",
    "                            sum=0\n",
    "                            for connection in range(num_neuron[l-1]+1):#### loop previous connections\n",
    "                                if connection == 0 :\n",
    "                                    sum=sum+w[neuron,connection]\n",
    "                                else :\n",
    "                                    sum=sum+temp_2[connection-1]*w[neuron,connection]##note col is weights that connect to this neuron\n",
    "\n",
    "                            if user_AvtivationFunction== 'Sigmoid':\n",
    "                                temp.append(Sigmoid(stander_division,sum)) ## calc z output of activation function\n",
    "                            else :\n",
    "                                temp.append(stander_division*Tanh(b,sum))\n",
    "\n",
    "                        z.append(temp)\n",
    "\n",
    "                ## calc y for output\n",
    "                y=[]\n",
    "                temp_2=z[-1]  ## get  z of last layer\n",
    "                \n",
    "                for output in range(3):\n",
    "                    w=ouput_w\n",
    "                    sum=0\n",
    "                    for connection in range(num_neuron[-1]+1):\n",
    "                        if connection == 0 :\n",
    "                            sum=sum+w[output,connection]\n",
    "                        else :\n",
    "                            sum=sum+temp_2[connection-1]*w[output,connection]\n",
    "                            \n",
    "                    if user_AvtivationFunction== 'Sigmoid':\n",
    "                        y.append(Sigmoid(stander_division,sum))\n",
    "                    else :\n",
    "                        y.append(stander_division*Tanh(b,sum))\n",
    "\n",
    "                \n",
    "                ##################################### back-propagation  #####################################\n",
    "\n",
    "\n",
    "                ## calc sk for output\n",
    "                sk=[]\n",
    "\n",
    "                for j in range(3):\n",
    "                    if user_AvtivationFunction== 'Sigmoid':\n",
    "                        sk.append(  stander_division*(target[j]-y[j])*y[j]*(1-y[j]) )  ## for segmoid function\n",
    "                    else :\n",
    "                        sk.append( (b/stander_division)*(target[j]-y[j])*(stander_division-y[j])\n",
    "                                                       *(stander_division+y[j]))\n",
    "\n",
    "\n",
    "                ## calc sj for hidden layers\n",
    "                sj=[]\n",
    "                for layer in range(num_layers,0,-1): ## calc sj for each neurons in last hidden layers\n",
    "\n",
    "                    if layer == num_layers : ## layer connent to output \n",
    "                        temp=[]\n",
    "                        temp_2=z[num_layers-1]\n",
    "                        for neuron in range(num_neuron[num_layers-1]):#calc sj for each neuron in last layer\n",
    "                            zj=temp_2[neuron]\n",
    "                            w=ouput_w\n",
    "                            sum=0                            \n",
    "                            for conn in range(3):\n",
    "                                sum=sum+w[conn,neuron+1]*sk[conn]\n",
    "       \n",
    "                            if user_AvtivationFunction== 'Sigmoid':\n",
    "                                temp.append( stander_division*zj*(1-zj)*sum ) ## sigmoid activation\n",
    "                            else :\n",
    "                                temp.append((b/stander_division)*(stander_division-zj)\n",
    "                                            *(stander_division+zj)* sum) ## tanh activation\n",
    "                            \n",
    "                        sj.append(temp)\n",
    "\n",
    "                    else :\n",
    "                        temp=[]\n",
    "                        temp_2=z[layer-1]\n",
    "                        temp_3=sj[num_layers-layer-1]\n",
    "                        for neuron in range(num_neuron[layer-1]):#calc sj for each neuron in layer\n",
    "                            zj=temp_2[neuron]\n",
    "\n",
    "                            w=hidden_w[layer]\n",
    "                            \n",
    "                            sum=0\n",
    "                            for conn in range(num_neuron[layer]):\n",
    "                                sum=sum+w[conn,neuron]*temp_3[conn]\n",
    "                                \n",
    "                            if user_AvtivationFunction== 'Sigmoid':\n",
    "                                temp.append( stander_division*zj*(1-zj)*sum ) ## sigmoid activation\n",
    "                            else :\n",
    "                                temp.append((b/stander_division)*(stander_division-zj)\n",
    "                                            *(stander_division+zj)* sum) ## tanh activation\n",
    "\n",
    "                        sj.append(temp)\n",
    "\n",
    "                ##################################### feedforward (updata weights) 2  #########################\n",
    "                for layer in range(num_layers+1): ##loop at every layer\n",
    "                    if layer == num_layers:\n",
    "                        w=ouput_w\n",
    "                        temp_2=z[num_layers-1]\n",
    "                        for neuron in range(num_neuron[num_layers-1]):# for each neuron in last layer\n",
    "                            \n",
    "                            for conn in range(3):\n",
    "                                if neuron == 0:\n",
    "                                    ouput_w[conn,neuron]= w[conn,neuron]+learning_rate*sk[conn]*1\n",
    "                                else :\n",
    "                                    ouput_w[conn,neuron]= w[conn,neuron]+learning_rate*sk[conn]*temp_2[neuron]\n",
    "\n",
    "\n",
    "                    elif layer == 0 :\n",
    "                        w=hidden_w[layer]\n",
    "                        temp=sj[num_layers-layer-1]  ## because sj in order inverse \n",
    "                        for neuron in range(num_neuron[layer]):# for each neuron in last layer\n",
    "                            w[0,neuron]=w[0,neuron]+learning_rate*temp[neuron]*1\n",
    "                            w[1,neuron]=w[1,neuron]+learning_rate*temp[neuron]*x1\n",
    "                            w[2,neuron]=w[2,neuron]+learning_rate*temp[neuron]*x2\n",
    "                            w[3,neuron]=w[3,neuron]+learning_rate*temp[neuron]*x3\n",
    "                            w[4,neuron]=w[4,neuron]+learning_rate*temp[neuron]*x4\n",
    "                        hidden_w[layer]=w\n",
    "\n",
    "                    else :\n",
    "                        w=hidden_w[layer]\n",
    "                        temp_2=z[layer-1]\n",
    "                        temp=sj[num_layers-layer-1]\n",
    "                        #print(temp,\" sj \")\n",
    "                        #print(temp_2,\" z \")\n",
    "                        #print(w)\n",
    "                        for neuron in range(num_neuron[layer]):# for each neuron in hidden layer\n",
    "\n",
    "                            for conn in range(num_neuron[layer-1]):\n",
    "                                #print(conn)\n",
    "                                if conn==0 :\n",
    "                                    w[neuron,conn]=w[neuron,conn]+learning_rate*temp[neuron]*1\n",
    "                                else :\n",
    "                                    w[neuron,conn]=w[neuron,conn]+learning_rate*temp[neuron]*temp_2[conn]\n",
    "                                    \n",
    "                        hidden_w[layer]=w\n",
    "            \n",
    "    return hidden_w,ouput_w         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "716c2ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateConfuionMatrix(actual_output, prediction):\n",
    "    \"\"\"\n",
    "    Purpose: Create Confusion Matrix and Print output status\n",
    "    :param actual_output: the real values of tested data\n",
    "    :param prediction: the calculated output of trained parameters\n",
    "    :return: accuracy\n",
    "    \"\"\"\n",
    "    # Defined Variables\n",
    "    columns = ['actual', 'prediction', 'status']\n",
    "    row = list()\n",
    "    test_status = pd.DataFrame(columns=columns)\n",
    "    confusion_matrix = np.zeros([2, 2])\n",
    "\n",
    "    # Creating Confusion Matrix Process and Get status of output\n",
    "    for i in range(len(prediction)):\n",
    "        if actual_output[i] == 1:\n",
    "            if prediction[i] == 1:\n",
    "                confusion_matrix[0][0] += 1\n",
    "                row.append([actual_output[i], prediction[i], 'Matching'])\n",
    "            else:\n",
    "                confusion_matrix[0][1] += 1\n",
    "                row.append([actual_output[i], prediction[i], 'Mismatching'])\n",
    "\n",
    "        elif actual_output[i] == -1:\n",
    "            if prediction[i] == -1:\n",
    "                confusion_matrix[1][1] += 1\n",
    "                row.append([actual_output[i], prediction[i], 'Matching'])\n",
    "            else:\n",
    "                confusion_matrix[1][0] += 1\n",
    "                row.append([actual_output[i], prediction[i], 'Mismatching'])\n",
    "\n",
    "    # Then Print status of each output\n",
    "    rows = pd.DataFrame(row, columns=columns)\n",
    "    test_status = test_status.append(rows, ignore_index=True)\n",
    "    print(\"--> The Test is Now Running ...\")\n",
    "    print(test_status)\n",
    "\n",
    "    # Calculate accuracy by [sum of diagonal / total sum]\n",
    "    accuracy = np.trace(confusion_matrix) / np.sum(confusion_matrix)\n",
    "\n",
    "    # Show Confusion Matrix\n",
    "    print(\"--> Confusion Matrix:\")\n",
    "    print(confusion_matrix)\n",
    "\n",
    "    # return the accuracy\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "23fdc077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avtivation   LR   Epochs Layer HiddenNodes\n",
      "  Hyperbolic    0.4     1000     2     [7, 4]\n",
      "\n",
      "\n",
      "Train accuracy       Test accuracy\n",
      "0.6666666666666666     0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "def CreateForm(size, title):\n",
    "    form = Tk()\n",
    "    form.geometry(size)\n",
    "    form.title(title)\n",
    "    form.resizable(False, False)\n",
    "    return form\n",
    "\n",
    "def CreateLabel(text):\n",
    "    lbl_var = StringVar()\n",
    "    lbl_var.set(text)\n",
    "    lbl_features = Label(master=master, textvariable=lbl_var, font=fontStyle)\n",
    "    lbl_features.pack(fill='x', padx=5, pady=5)\n",
    "\n",
    "def CreateComboBox(data):\n",
    "    cmb_obj = ttk.Combobox(master=master, textvariable=StringVar(), font=fontStyle)\n",
    "    cmb_obj['values'] = data\n",
    "    cmb_obj['state'] = 'readonly'\n",
    "    cmb_obj.pack(fill='x', padx=5, pady=5)\n",
    "    return cmb_obj\n",
    "\n",
    "def CreateTextbox():\n",
    "    txt_entry = Entry(master, font=fontStyle)\n",
    "    txt_entry.pack(fill='x', padx=5, pady=5)\n",
    "    return txt_entry\n",
    "\n",
    "def CreateCheckbox(text):\n",
    "    cb_var = IntVar()\n",
    "    cb_obj = Checkbutton(master=master, text=text, variable=cb_var, onvalue=1, offvalue=0, font=fontStyle)\n",
    "    cb_obj.pack()\n",
    "    return cb_var\n",
    "\n",
    "def GetSelectedAvtivatioFunction():\n",
    "    selected_option = classes_cmb.get()\n",
    "    return selected_option\n",
    "\n",
    "def GetLearningRate():\n",
    "    return float(txt_LearningRate.get())\n",
    "\n",
    "def GetNumberOfHiddenLayer():\n",
    "    return int(txt_Number_Hidden_Layer.get())\n",
    "\n",
    "def GetNumberOfNeuronPerHiddenLayer():\n",
    "    NNPH = txt_Num_Neurons_Per_HiddenLayer.get()\n",
    "    if ',' in NNPH:\n",
    "        num = [int(x)for x in NNPH.split(',')]\n",
    "    else:\n",
    "        num = int(NNPH)\n",
    "    return num\n",
    "\n",
    "def GetEpochNumber():\n",
    "    return int(txt_Epochs.get())\n",
    "\n",
    "\n",
    "def GetBiasDesicion():\n",
    "    return int(use_bias.get())\n",
    "\n",
    "def RunWholeProgram():\n",
    "    ###### reading data #####\n",
    "    dataset = pd.read_csv('E:\\\\IrisData.txt')\n",
    "\n",
    "    class_a, class_b, class_c = dataset[:50], dataset[50:100], dataset[100:150]\n",
    "\n",
    "    class_a_train, class_b_train, class_c_train = class_a[:30], class_b[:30], class_c[:30]\n",
    "    class_a_test, class_b_test, class_c_test = class_a[30:], class_b[30:], class_c[30:]\n",
    "\n",
    "    train_data = pd.concat([class_a_train, class_b_train, class_c_train])\n",
    "    test_data = pd.concat([class_a_test, class_b_test, class_c_test])\n",
    "\n",
    "    train_data = train_data.sample(frac=1)\n",
    "    train_data=shuffle(train_data)\n",
    "    test_data = test_data.sample(frac=1)\n",
    "    ########################\n",
    "    \n",
    "    # Defined Variables\n",
    "    \n",
    "    # Get Results From Form\n",
    "    user_AvtivationFunction = GetSelectedAvtivatioFunction()\n",
    "    user_lr = GetLearningRate()\n",
    "    user_epochs = GetEpochNumber()\n",
    "    user_bias = GetBiasDesicion()\n",
    "    numberOfHiddenLayers = GetNumberOfHiddenLayer()\n",
    "    numberOfNeuronsPerHiddenLayer = GetNumberOfNeuronPerHiddenLayer()  \n",
    "    \n",
    "    \n",
    "    #####  choice ##########\n",
    "    if user_bias == 1 :\n",
    "        bais=True\n",
    "    else :\n",
    "        bais=False\n",
    "       \n",
    "    activation_choice=user_AvtivationFunction  ## \n",
    "    ########################\n",
    "\n",
    "    ###### Initialize the weights #####\n",
    "    num_layers=numberOfHiddenLayers\n",
    "    num_neuron=numberOfNeuronsPerHiddenLayer\n",
    "    \n",
    "    ###hyper_parameter ###\n",
    "    stander_division=.5#need for sigmoid\n",
    "    epoches=user_epochs\n",
    "    b=.3\n",
    "    learning_rate=user_lr\n",
    "    ######################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    hidden_w,ouput_w=Initialize_weights(num_layers,num_neuron,bais)\n",
    "    \n",
    "    #print(hidden_w,ouput_w)\n",
    "    model_W_hidden,model_w_output=back_propagation(stander_division,epoches,train_data,b,learning_rate,hidden_w,\n",
    "                                        ouput_w,bais,num_layers,num_neuron,user_AvtivationFunction)\n",
    "    \n",
    "    #print('<<<<<<<<<<<<<<<<<<<<<<<<<<<<')\n",
    "    #print(model_W_hidden,model_w_output)\n",
    "    \n",
    "    \n",
    "    s=0\n",
    "    for i in range((int(train_data.size/5))):\n",
    "        sample=train_data[i:i+1]\n",
    "        target=decode(sample['Class'].min())\n",
    "        y=predicetion(model_W_hidden,model_w_output,sample,bais,\n",
    "                          user_AvtivationFunction,b,num_layers,num_neuron,stander_division)\n",
    "        if y == target :\n",
    "            s=s+1\n",
    "        \n",
    "    train_accuracy=(s/(int(train_data.size/5)))\n",
    "    #print('Train accuracy',train_accuracy )  \n",
    "        \n",
    "    sum=0\n",
    "    actual_output=[]\n",
    "    prediction_value=[]\n",
    "    for i in range((int(test_data.size/5))):#(int(test_data.size/5))\n",
    "        sample=test_data[i:i+1]\n",
    "\n",
    "        target=decode(sample['Class'].min())\n",
    "        actual_output.append(target)\n",
    "        y=predicetion(model_W_hidden,model_w_output,sample,bais,\n",
    "                      user_AvtivationFunction,b,num_layers,num_neuron,stander_division)\n",
    "        prediction_value.append(y)\n",
    "        #print(y)   \n",
    "        if y == target :\n",
    "            sum=sum+1\n",
    "         \n",
    "    test_accuracy=(sum/(int(test_data.size/5))) \n",
    "    #print('test accuracy',test_accuracy)\n",
    "    #accuracy=CreateConfuionMatrix(actual_output, prediction_value)\n",
    "    #print(accuracy)\n",
    "    print('Avtivation',' ','LR',' ','Epochs','Layer','HiddenNodes' )\n",
    "    print(' ',user_AvtivationFunction,'  ',learning_rate,\n",
    "          '   ',user_epochs,'   ',numberOfHiddenLayers,'   ',num_neuron)\n",
    "    print()\n",
    "    print()\n",
    "    print('Train accuracy','     ','Test accuracy')\n",
    "    print(train_accuracy,'   ',test_accuracy)\n",
    "####################### Start of Main ###########################\n",
    "# Create main form\n",
    "master = CreateForm(size=\"350x450\", title=\"TASK 2 SOLUTION\")\n",
    "fontStyle = tkFont.Font(family=\"JetBrains Mono\", size=10)\n",
    "\n",
    "\n",
    "# Create Label and Combobox for Classes\n",
    "classes = ('Sigmoid', 'Hyperbolic')\n",
    "CreateLabel(text=\"Select activation function\")\n",
    "classes_cmb = CreateComboBox(data=classes)\n",
    "\n",
    "# Create Label and TextBox for numberOfHiddenLayer\n",
    "CreateLabel(text=\"Enter number of hidden layers\")\n",
    "txt_Number_Hidden_Layer = CreateTextbox()\n",
    "\n",
    "# Create Label and TextBox for N of Neuron per hidden Layer\n",
    "CreateLabel(text=\"Enter number of neurons per hidden layer\")\n",
    "txt_Num_Neurons_Per_HiddenLayer = CreateTextbox()\n",
    "\n",
    "# Create Label and TextBox for Learning Rate\n",
    "CreateLabel(text=\"Enter Learning rate Value\")\n",
    "txt_LearningRate = CreateTextbox()\n",
    "\n",
    "# Create Label and TextBox for Epochs\n",
    "CreateLabel(text=\"Enter Epochs Number\")\n",
    "txt_Epochs = CreateTextbox()\n",
    "\n",
    "# Create Label and Checkbox for bias\n",
    "CreateLabel(text=\"Please Check this for Selecting Bias\")\n",
    "use_bias = CreateCheckbox(text=\"Use Bias\")\n",
    "\n",
    "# Create Button\n",
    "btn_submit = Button(master=master, text=\"Submit\", width=15, command=RunWholeProgram, font=fontStyle)\n",
    "btn_submit.pack(pady=15)\n",
    "\n",
    "# Run the Form\n",
    "master.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b03b57e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
